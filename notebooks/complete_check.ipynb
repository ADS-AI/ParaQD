{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers==1.2.0\n",
    "!pip --quiet install --upgrade transformers sentencepiece sentence-splitter strsim num2words sentence-transformers\n",
    "!pip --quiet install git+https://github.com/LIAAD/yake python-rake sentence-splitter wordtodigits strsim\n",
    "!pip install ax-platform==0.1.9\n",
    "!pip install mlrose\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip --quiet install stemming quantulum3 scikit-learn==0.24.2 inflect\n",
    "!pip install strsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "from sentence_splitter import SentenceSplitter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "import pke\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import random\n",
    "import spacy\n",
    "import wordtodigits\n",
    "import pickle\n",
    "import inflect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for augmenting a given text.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Corrupts the given text.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTranslate(Operation):\n",
    "    def __init__(self):\n",
    "        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(\"self.DEVICE\", self.DEVICE)\n",
    "        forward_tokenizer_path = \"/content/drive/MyDrive/resources/models/FSMT/en-de/tokenizer\"\n",
    "        forward_model_path = \"/content/drive/MyDrive/resources/models/FSMT/en-de/model\"\n",
    "        if not os.path.exists(forward_tokenizer_path):\n",
    "            forward_tokenizer_path = forward_model_path = \"facebook/wmt19-en-de\"\n",
    "        backward_tokenizer_path = \"/content/drive/MyDrive/resources/models/FSMT/de-en/tokenizer\"\n",
    "        backward_model_path = \"/content/drive/MyDrive/resources/models/FSMT/de-en/model\"\n",
    "        if not os.path.exists(backward_tokenizer_path):\n",
    "            backward_tokenizer_path = backward_model_path = \"facebook/wmt19-de-en\"\n",
    "        print(\"[INFO] Loading en-de model...\")\n",
    "        self.forward_tokenizer = FSMTTokenizer.from_pretrained(forward_tokenizer_path)\n",
    "        self.forward_model = FSMTForConditionalGeneration.from_pretrained(forward_model_path).to(self.DEVICE)\n",
    "        print(\"[INFO] Loading de-en model...\")\n",
    "        self.backward_tokenizer = FSMTTokenizer.from_pretrained(backward_tokenizer_path)\n",
    "        self.backward_model = FSMTForConditionalGeneration.from_pretrained(backward_model_path).to(self.DEVICE)\n",
    "        self.splitter = SentenceSplitter(language='en')\n",
    "\n",
    "    def beam_search(self, text, tokenizer, model, num_beams=10):\n",
    "        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(self.DEVICE)\n",
    "        beam_output = model.generate(\n",
    "            input_ids, \n",
    "            num_beams=num_beams, \n",
    "            early_stopping=True)\n",
    "        text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "    def diverse_beam_search(self, text, tokenizer, model, beam_groups=4, num_return_sequences=4, diversity_penalty=1.5):\n",
    "        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(self.DEVICE)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids,\n",
    "            num_beams = beam_groups*3,\n",
    "            num_return_sequences = num_return_sequences,\n",
    "            diversity_penalty = diversity_penalty,\n",
    "            num_beam_groups = beam_groups)\n",
    "        text = []\n",
    "        for beam_output in beam_outputs:\n",
    "            text.append(tokenizer.decode(beam_output, skip_special_tokens=True))\n",
    "        return text\n",
    "\n",
    "    def backtranslate_single(self, text):\n",
    "        nl = NormalizedLevenshtein()\n",
    "        t_split = sent_tokenize(text)\n",
    "        paraphrased = [\"\"]\n",
    "        for sentence in t_split:\n",
    "            temp = []\n",
    "            translated = self.beam_search(sentence, self.forward_tokenizer, self.forward_model, num_beams=8)\n",
    "            paraphrases = self.diverse_beam_search(translated, self.backward_tokenizer, self.backward_model, num_return_sequences=2, beam_groups=4, diversity_penalty=50.0)\n",
    "            for paraphrase in paraphrases:\n",
    "                for prev in paraphrased:\n",
    "                    temp.append(prev+paraphrase)\n",
    "            paraphrased = temp\n",
    "        paraphrased = [(paraphrase, nl.distance(text, paraphrase)) for paraphrase in paraphrased]\n",
    "        paraphrase = sorted(paraphrased, key=lambda x: x[1], reverse=True)[0]\n",
    "        return paraphrase[0]\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        return self.backtranslate_single(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeleteLastSentence(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        text_split = sent_tokenize(text)\n",
    "        if len(text_split) > 1:\n",
    "            text = \" \".join(text_split[:-1])\n",
    "            return text\n",
    "        return \" \".join(text.split()[:-4]) # Deletes last 4 tokens if there is only 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MostImportantPhraseRemover(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.sentence_transformer = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "\n",
    "    def lose_most_important_word(self, text, soften=False):\n",
    "        tokenized = text.split()\n",
    "        original_embedding = self.sentence_transformer.encode([text], convert_to_tensor=True)\n",
    "        min_score = 1; final_text = \"\"\n",
    "        processed_sentences = []\n",
    "        keyphrases = []\n",
    "\n",
    "        try:\n",
    "            self.extractor = pke.unsupervised.TopicRank()\n",
    "            self.extractor.load_document(input=text, language='en')\n",
    "            self.extractor.candidate_selection()\n",
    "            self.extractor.candidate_weighting()\n",
    "            keyphrases = self.extractor.get_n_best(n=4)\n",
    "            keyphrases = list(map(lambda x: x[0], keyphrases))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if len(keyphrases) > 0:\n",
    "            for keyphrase in keyphrases:\n",
    "                processed_text = text.replace(keyphrase, \" \")\n",
    "                processed_text = re.sub(\"\\s+\", \" \", processed_text)\n",
    "                processed_sentences.append(processed_text)\n",
    "        \n",
    "        else:\n",
    "            for idx, word in enumerate(tokenized):\n",
    "                processed_text = \" \".join(tokenized[:idx]+tokenized[idx+1:])\n",
    "                processed_text = re.sub(\"\\s+\", \" \", processed_text)\n",
    "                processed_sentences.append(processed_text)\n",
    "\n",
    "        processed_embeddings = self.sentence_transformer.encode(processed_sentences, convert_to_tensor=True)\n",
    "        cosine_scores = util.pytorch_cos_sim(original_embedding, processed_embeddings)\n",
    "        idx = np.argmin(cosine_scores[0].cpu().numpy())\n",
    "        return processed_sentences[idx]\n",
    "\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        return self.lose_most_important_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Num2Words(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        numbers = list(set(re.findall(\"\\d+\\.?\\d*\", text)))\n",
    "        mapping = {number: num2words(number) for number in numbers}\n",
    "        for number in mapping:\n",
    "            text = text.replace(number, mapping[number])\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        text = \" \".join(sentence[0].upper() + sentence[1:] for sentence in sentences)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pegasus(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        model_name = 'tuner007/pegasus_paraphrase'\n",
    "        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.model = PegasusForConditionalGeneration.from_pretrained(model_name).to(self.DEVICE)\n",
    "        self.nl = NormalizedLevenshtein()\n",
    "\n",
    "    def get_response(self, input_text, num_return_sequences=5, beam_groups=5):\n",
    "        batch = self.tokenizer(input_text, max_length=128, truncation=True, return_tensors=\"pt\").to(self.DEVICE)\n",
    "        translated = self.model.generate(**batch, max_length=128, \n",
    "                            num_beam_groups = beam_groups, diversity_penalty=5.0, \n",
    "                            num_beams=beam_groups*2, num_return_sequences=num_return_sequences, temperature=0.5)\n",
    "        tgt_text = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        return tgt_text\n",
    "\n",
    "    def run_pegasus(self, text, nl=NormalizedLevenshtein(), n=2, soften=False):\n",
    "        num = n+1 if n>5 else 5\n",
    "        pegasus_outputs = list(set(self.get_response(text, num_return_sequences=num)))\n",
    "        pegasus_outputs = [(output, nl.distance(output, text)) for output in pegasus_outputs]\n",
    "        pegasus_outputs = sorted(pegasus_outputs, key=lambda x: x[1], reverse=True)[:n]\n",
    "        for pegasus_output, dist in pegasus_outputs:\n",
    "            if len(pegasus_output.split()) > len(text.split())//2:\n",
    "                return pegasus_output\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        return self.run_pegasus(text, nl=self.nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDeletion(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        soften = kwargs.get(\"soften\", False)\n",
    "        words = text.split()\n",
    "        num = np.random.randint(1, max(4, len(words)//10))\n",
    "        if soften:\n",
    "            num = 1\n",
    "        idxs = np.random.randint(0, len(words), size=num)\n",
    "        text = \" \".join([words[i] for i in range(len(words)) if i not in idxs])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaceNamedEntities(Operation):\n",
    "    def __init__(self, resource_dir) -> None:\n",
    "        super().__init__()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        fpaths = [os.path.join(resource_dir, fpath) for fpath in os.listdir(resource_dir) if fpath.endswith(\".txt\")]\n",
    "        self.resources = [open(fpath, \"r\").read().splitlines() for fpath in fpaths]\n",
    "\n",
    "    def get_replacement(self, entity):\n",
    "        possibilities = []\n",
    "        for resource in self.resources:\n",
    "            if entity.lower() in list(map(lambda x: x.lower(), resource)):\n",
    "                possibilities.extend(random.sample(resource, 5))\n",
    "                break\n",
    "        possibilities = [\" {} \".format(x) for x in possibilities]\n",
    "        possibilities.append(\" \")\n",
    "        return random.choice(possibilities)\n",
    "\n",
    "    def replace_named_entities(self, text, soften=False):\n",
    "        \"\"\"\n",
    "        loses persons, organizations, products and places\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        named_entities = set([\"PERSON\", \"ORG\", \"PRODUCT\", \"EVENT\", \"GPE\", \"GEO\"])\n",
    "        ne = []\n",
    "        for x in doc.ents:\n",
    "            if x.label_ in named_entities:\n",
    "                ne.append((x.text, x.start_char, x.end_char))\n",
    "        if len(ne) == 0:\n",
    "            return text\n",
    "        ne_new = random.sample(ne, np.random.randint(1, min(len(ne), 3)+1))\n",
    "        ne_new = sorted(ne_new, key=lambda x: x[1])\n",
    "        if soften:\n",
    "            ne_new = ne_new[:1]\n",
    "        shift = 0\n",
    "        for (entity, start, end) in ne_new:\n",
    "            replacemnt = self.get_replacement(entity)\n",
    "            text = text[:start-shift] + replacemnt + text[end-shift:]\n",
    "            shift += end - start - len(replacemnt)\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        soften = kwargs.get(\"soften\", False)\n",
    "        return self.replace_named_entities(text, soften)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaceNumericalEntities(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        possibilities = [\"some\", \"a few\", \"many\", \"a lot of\", \"\"]\n",
    "        replacement = np.random.choice(possibilities)\n",
    "        text = wordtodigits.convert(text)\n",
    "        numbers = list(set(re.findall(\"\\d+\\.?\\d*\", text)))\n",
    "        numbers = np.random.choice(numbers, np.random.randint(1, min(len(numbers)+1, 3)), replace=False)\n",
    "        for number in numbers:\n",
    "            text = re.sub(number, \" \"+replacement+\" \", text)\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaceUnits(Operation):\n",
    "    def __init__(self):\n",
    "        currencies = [\"dollar\", \"cent\", \"nickel\", \"penny\", \"quarter\", \"dime\", \"rupee\", \"paisa\", \"pound\", \"euro\"]\n",
    "        currency_abbrv = {}\n",
    "        currency_symbs = [\"$\", \"£\", \"₹\", \"€\"]\n",
    "        currency_symbs_abbrv = {}\n",
    "        currency_short = [\"rs\", \"USD\", \"EUR\", \"GBP\"]\n",
    "        currency_short_abbrv = {}\n",
    "        length = [\"inch\", \"meter\", \"metre\", \"kilometre\", \"kilometer\", \"centimeter\", \"millimeter\", \"millimetre\", \"centimetre\", \"foot\", \"mile\"]\n",
    "        length_abbrv = {\"m\": \"metre\", \"km\": \"kilometre\", \"cm\":\"centimetre\", \"mm\":\"millimetre\", \"ft\": \"foot\", \"mil\": \"mile\"}\n",
    "        weight = [\"gram\", \"kilogram\"]\n",
    "        weight_abbrv = {\"g\": \"gram\", \"gm\": \"gram\", \"kg\": \"kilogram\"}\n",
    "        time = [\"minute\", \"hour\", \"day\", \"year\", \"month\", \"week\"]\n",
    "        time_abbrv = {\"min\": \"minute\", \"hr\": \"hour\", \"yr\": \"year\", \"wk\": \"week\"}\n",
    "        speed = [\"kilometre per hour\", \"miles per hour\", \"metre per second\"]\n",
    "        speed_abbrv = {\"km/hr\": \"kilometre per hour\", \"km/h\": \"kilometre per hour\", \"kmph\": \"kilometre per hour\", \"mph\": \"miles per hour\", \"m/s\": \"metre per second\", \"mps\": \"metre per second\"}\n",
    "        p = inflect.engine()\n",
    "        self.full = [currencies, currency_symbs, currency_short, length, weight, time, speed]\n",
    "        self.plurals = [[p.plural(unit) for unit in unit_type] for unit_type in self.full] + [\"seconds\"]\n",
    "        self.abbreviated = [currency_abbrv, currency_symbs_abbrv, currency_short_abbrv, length_abbrv, weight_abbrv, time_abbrv, speed_abbrv]\n",
    "        self.abbreviated_pl = [{p.plural(key): p.plural(value) for (key, value) in ab_dict.items()} for ab_dict in self.abbreviated]\n",
    "        self.nl = NormalizedLevenshtein()\n",
    "\n",
    "    def find_closest(self, word, wordlist):\n",
    "        if word in wordlist:\n",
    "            return word\n",
    "        closest = min([(w, self.nl.distance(word, w)) for w in wordlist], key=lambda x: x[1])\n",
    "        return closest[0]\n",
    "\n",
    "    def untokenize(self, words):\n",
    "        \"\"\"\n",
    "        ref: https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py#L28\n",
    "        \"\"\"\n",
    "        text = \" \".join(words)\n",
    "        step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace(\". . .\", \"...\")\n",
    "        step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "        step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "        step4 = re.sub(r\" ([.,:;?!%]+)$\", r\"\\1\", step3)\n",
    "        step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\"can not\", \"cannot\")\n",
    "        step6 = step5.replace(\" ` \", \" '\")\n",
    "        return step6.strip()\n",
    "\n",
    "    def abbreviate(self, word, abbrvs):\n",
    "        closest_word = self.find_closest(word, abbrvs.values())\n",
    "        for a, w in abbrvs.items():\n",
    "            if closest_word == w:\n",
    "                return a\n",
    "        return word\n",
    "\n",
    "    def find_replacement_full_index(self, unit, index, negative=True):\n",
    "        unit_list = self.full[index]\n",
    "        unit_list_pl = self.plurals[index]\n",
    "        unit = unit.lower().strip()\n",
    "        \n",
    "        if unit in unit_list:\n",
    "            replacement = np.random.choice(unit_list)\n",
    "            if negative:\n",
    "                while replacement == unit or self.nl.distance(unit, replacement)<0.2:\n",
    "                    replacement = np.random.choice(unit_list)\n",
    "            else:\n",
    "                if unit == \"rs\": return \"Rupees\"\n",
    "                replacement = unit\n",
    "            return replacement\n",
    "            \n",
    "        if unit in unit_list_pl:\n",
    "            replacement = np.random.choice(unit_list_pl)\n",
    "            if negative:\n",
    "                while replacement == unit or self.nl.distance(unit, replacement)<0.2:\n",
    "                    replacement = np.random.choice(unit_list)\n",
    "            else:\n",
    "                replacement = unit\n",
    "            return replacement\n",
    "        return \"\"\n",
    "\n",
    "    def find_replacement_full(self, unit, negative=True):\n",
    "        for index in range(len(self.full)):\n",
    "            replacement = self.find_replacement_full_index(unit, index, negative)\n",
    "            if replacement:\n",
    "                return replacement\n",
    "        return \"\"\n",
    "\n",
    "    def find_replacement_abbreviated(self, unit, negative=True):\n",
    "        unit = unit.strip().lower()\n",
    "\n",
    "        for i, abb_type in enumerate(self.abbreviated):\n",
    "            abb_type_pl = self.abbreviated_pl[i]\n",
    "            if unit in abb_type:\n",
    "                unit_full = abb_type[unit]\n",
    "                replacement = self.find_replacement_full_index(unit_full, i, negative=negative)\n",
    "                if negative:\n",
    "                    replacement = self.abbreviate(replacement, abb_type)\n",
    "                return replacement\n",
    "\n",
    "            if unit in abb_type_pl:\n",
    "                unit_full = abb_type_pl[unit]\n",
    "                replacement = self.find_replacement_full_index(unit_full, i, negative=negative)\n",
    "                if negative:\n",
    "                    replacement = self.abbreviate(replacement, abb_type_pl)\n",
    "                return replacement\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def find_replacement(self, unit, negative=True, abbreviated=False):\n",
    "        if abbreviated:\n",
    "            return self.find_replacement_abbreviated(unit, negative)\n",
    "        replacement = self.find_replacement_full(unit, negative)\n",
    "        if replacement:\n",
    "            return replacement\n",
    "        return self.find_replacement_abbreviated(unit, negative)\n",
    "\n",
    "    def change(self, sent, negative=True):\n",
    "        original_sent = sent\n",
    "        tokenized = word_tokenize(sent)\n",
    "        replaced = False\n",
    "\n",
    "        for i, token in enumerate(tokenized):\n",
    "            quants = re.findall(\"\\d+([a-z/]{1,5})\", token)\n",
    "            if quants:\n",
    "                if replaced and np.random.choice([True, False], p=[0.65, 0.35]): break\n",
    "                unit = quants[0]\n",
    "                replacement = self.find_replacement(unit, abbreviated=True, negative=negative)\n",
    "                if replacement == \"\":\n",
    "                    continue\n",
    "                token_new = token.replace(unit, replacement)\n",
    "                tokenized[i] = token_new\n",
    "                replaced = True    \n",
    "            else:\n",
    "                replacement = self.find_replacement(token, negative=negative)\n",
    "                if replacement:\n",
    "                    if replaced and np.random.choice([True, False], p=[0.65, 0.35]): break\n",
    "                    tokenized[i] = replacement\n",
    "                    replaced = True\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        sent = self.untokenize(tokenized)\n",
    "        if sent == original_sent:\n",
    "            sent = \"\"\n",
    "        return sent\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        return self.change(text, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameSentence(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF_Replacement(Operation):\n",
    "    def __init__(self, resource_dir) -> None:\n",
    "        super().__init__()\n",
    "        tfidf_path = os.path.join(resource_dir, \"tfidf_aqua.pkl\")\n",
    "        self.tfidf = pickle.load(open(tfidf_path, \"rb\"))\n",
    "        self.words = self.tfidf.get_feature_names()\n",
    "\n",
    "    def __sample(self, n=3):\n",
    "        return random.sample(self.words, n)\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        soften = kwargs.get(\"soften\", False)\n",
    "        \n",
    "        transformed = self.tfidf.transform([text]).toarray()\n",
    "        most_imp = np.argpartition(transformed, -4)[:, -4:]\n",
    "        array = most_imp[0]\n",
    "        question = text\n",
    "        vals = []\n",
    "        num_replace = np.random.randint(1, 3)\n",
    "        if soften:\n",
    "            num_replace = 1\n",
    "        replacements = self.__sample(num_replace)\n",
    "        for idx in array:\n",
    "            val = transformed[0][idx]\n",
    "            word = self.words[idx]\n",
    "            vals.append((val, word))\n",
    "        vals.sort(reverse = True)\n",
    "        replaced = list(map(lambda x: x[1], vals))[:num_replace]\n",
    "        for replaced_, replacement in zip(replaced, replacements):\n",
    "            question = question.replace(replaced_, replacement, 1)\n",
    "        return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitExpansion(Operation):\n",
    "    def __init__(self):\n",
    "        currencies = [\"dollar\", \"cent\", \"nickel\", \"penny\", \"quarter\", \"dime\", \"rupee\", \"paisa\", \"pound\", \"euro\"]\n",
    "        currency_abbrv = {}\n",
    "        currency_symbs = [\"$\", \"£\", \"₹\", \"€\"]\n",
    "        currency_symbs_abbrv = {}\n",
    "        currency_short = [\"rs\", \"USD\", \"EUR\", \"GBP\"]\n",
    "        currency_short_abbrv = {}\n",
    "        length = [\"inch\", \"meter\", \"metre\", \"kilometre\", \"kilometer\", \"centimeter\", \"millimeter\", \"millimetre\", \"centimetre\", \"foot\", \"mile\"]\n",
    "        length_abbrv = {\"m\": \"metre\", \"km\": \"kilometre\", \"cm\":\"centimetre\", \"mm\":\"millimetre\", \"ft\": \"foot\", \"mil\": \"mile\"}\n",
    "        weight = [\"gram\", \"kilogram\"]\n",
    "        weight_abbrv = {\"g\": \"gram\", \"gm\": \"gram\", \"kg\": \"kilogram\"}\n",
    "        time = [\"minute\", \"hour\", \"day\", \"year\", \"month\", \"week\"]\n",
    "        time_abbrv = {\"min\": \"minute\", \"hr\": \"hour\", \"yr\": \"year\", \"wk\": \"week\"}\n",
    "        speed = [\"kilometre per hour\", \"miles per hour\", \"metre per second\"]\n",
    "        speed_abbrv = {\"km/hr\": \"kilometre per hour\", \"km/h\": \"kilometre per hour\", \"kmph\": \"kilometre per hour\", \"mph\": \"miles per hour\", \"m/s\": \"metre per second\", \"mps\": \"metre per second\"}\n",
    "        p = inflect.engine()\n",
    "        self.full = [currencies, currency_symbs, currency_short, length, weight, time, speed]\n",
    "        self.plurals = [[p.plural(unit) for unit in unit_type] for unit_type in self.full]\n",
    "        self.abbreviated = [currency_abbrv, currency_symbs_abbrv, currency_short_abbrv, length_abbrv, weight_abbrv, time_abbrv, speed_abbrv]\n",
    "        self.abbreviated_pl = [{p.plural(key): p.plural(value) for (key, value) in ab_dict.items()} for ab_dict in self.abbreviated]\n",
    "        self.nl = NormalizedLevenshtein()\n",
    "\n",
    "    def find_closest(self, word, wordlist):\n",
    "        if word in wordlist:\n",
    "            return word\n",
    "        closest = min([(w, self.nl.distance(word, w)) for w in wordlist], key=lambda x: x[1])\n",
    "        return closest[0]\n",
    "\n",
    "    def untokenize(self, words):\n",
    "        \"\"\"\n",
    "        ref: https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py#L28\n",
    "        \"\"\"\n",
    "        text = \" \".join(words)\n",
    "        step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace(\". . .\", \"...\")\n",
    "        step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "        step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "        step4 = re.sub(r\" ([.,:;?!%]+)$\", r\"\\1\", step3)\n",
    "        step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\"can not\", \"cannot\")\n",
    "        step6 = step5.replace(\" ` \", \" '\")\n",
    "        return step6.strip()\n",
    "\n",
    "    def abbreviate(self, word, abbrvs):\n",
    "        closest_word = self.find_closest(word, abbrvs.values())\n",
    "        for a, w in abbrvs.items():\n",
    "            if closest_word == w:\n",
    "                return a\n",
    "        return word\n",
    "\n",
    "    def find_replacement_full_index(self, unit, index, negative=True):\n",
    "        unit_list = self.full[index]\n",
    "        unit_list_pl = self.plurals[index]\n",
    "        unit = unit.lower().strip()\n",
    "        \n",
    "        if unit in unit_list:\n",
    "            replacement = np.random.choice(unit_list)\n",
    "            if negative:\n",
    "                while replacement == unit or self.nl.distance(unit, replacement)<0.2:\n",
    "                    replacement = np.random.choice(unit_list)\n",
    "            else:\n",
    "                if unit == \"rs\": return \"Rupees\"\n",
    "                replacement = unit\n",
    "            return replacement\n",
    "            \n",
    "        if unit in unit_list_pl:\n",
    "            replacement = np.random.choice(unit_list_pl)\n",
    "            if negative:\n",
    "                while replacement == unit or self.nl.distance(unit, replacement)<0.2:\n",
    "                    replacement = np.random.choice(unit_list)\n",
    "            else:\n",
    "                replacement = unit\n",
    "            return replacement\n",
    "        return \"\"\n",
    "\n",
    "    def find_replacement_full(self, unit, negative=True):\n",
    "        for index in range(len(self.full)):\n",
    "            replacement = self.find_replacement_full_index(unit, index, negative)\n",
    "            if replacement:\n",
    "                return replacement\n",
    "        return \"\"\n",
    "\n",
    "    def find_replacement_abbreviated(self, unit, negative=True):\n",
    "        unit = unit.strip().lower()\n",
    "\n",
    "        for i, abb_type in enumerate(self.abbreviated):\n",
    "            abb_type_pl = self.abbreviated_pl[i]\n",
    "            if unit in abb_type:\n",
    "                unit_full = abb_type[unit]\n",
    "                replacement = self.find_replacement_full_index(unit_full, i, negative=negative)\n",
    "                if negative:\n",
    "                    replacement = self.abbreviate(replacement, abb_type)\n",
    "                return replacement\n",
    "\n",
    "            if unit in abb_type_pl:\n",
    "                unit_full = abb_type_pl[unit]\n",
    "                replacement = self.find_replacement_full_index(unit_full, i, negative=negative)\n",
    "                if negative:\n",
    "                    replacement = self.abbreviate(replacement, abb_type_pl)\n",
    "                return replacement\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    def find_replacement(self, unit, negative=True, abbreviated=False):\n",
    "        if abbreviated:\n",
    "            return self.find_replacement_abbreviated(unit, negative)\n",
    "        replacement = self.find_replacement_full(unit, negative)\n",
    "        if replacement:\n",
    "            return replacement\n",
    "        return self.find_replacement_abbreviated(unit, negative)\n",
    "\n",
    "    def change(self, sent, negative=True):\n",
    "        original_sent = sent\n",
    "        tokenized = word_tokenize(sent)\n",
    "        replaced = False\n",
    "\n",
    "        for i, token in enumerate(tokenized):\n",
    "            quants = re.findall(\"\\d+([a-z/]{1,5})\", token)\n",
    "            if quants:\n",
    "                if replaced and np.random.choice([True, False], p=[0.65, 0.35]): break\n",
    "                unit = quants[0]\n",
    "                replacement = self.find_replacement(unit, abbreviated=True, negative=negative)\n",
    "                if replacement == \"\":\n",
    "                    continue\n",
    "                token_new = token.replace(unit, replacement)\n",
    "                tokenized[i] = token_new\n",
    "                replaced = True    \n",
    "            else:\n",
    "                replacement = self.find_replacement(token, negative=negative)\n",
    "                if replacement:\n",
    "                    if replaced and np.random.choice([True, False], p=[0.65, 0.35]): break\n",
    "                    tokenized[i] = replacement\n",
    "                    replaced = True\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        sent = self.untokenize(tokenized)\n",
    "        if sent == original_sent:\n",
    "            sent = \"\"\n",
    "        return sent\n",
    "\n",
    "    def generate(self, text, **kwargs):\n",
    "        return self.change(text, negative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedOperation(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, text, ops=[], **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositiveSamples(CombinedOperation):\n",
    "    def __init__(self):\n",
    "        self.operations = {\n",
    "            \"BackTranslate\": BackTranslate(),\n",
    "            \"SameSentence\": SameSentence(),\n",
    "            \"Num2Words\": Num2Words(),\n",
    "            \"UnitExpansion\": UnitExpansion()\n",
    "        }\n",
    "\n",
    "    def generate(self, text, ops=[\"BackTranslate\", \"SameSentence\", \"Num2Words\", \"UnitExpansion\"]):\n",
    "        positives = []\n",
    "        for op in ops:\n",
    "            if op not in self.operations: continue\n",
    "            operator = self.operations[op]\n",
    "            positives.append(operator.generate(text))\n",
    "        return positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamples(CombinedOperation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.operations = {\n",
    "            \"MostImportantPhraseRemover\": MostImportantPhraseRemover(),\n",
    "            \"DeleteLastSentence\": DeleteLastSentence(),\n",
    "            \"ReplaceNamedEntities\": ReplaceNamedEntities(),\n",
    "            \"ReplaceNumericalEntities\": ReplaceNumericalEntities(),\n",
    "            \"Pegasus\": Pegasus(),\n",
    "            \"ReplaceUnits\": ReplaceUnits(),\n",
    "            # \"NegateQuestion\": NegateQuestion(),\n",
    "        }\n",
    "        self.backup = TF_IDF_Replacement()\n",
    "\n",
    "    def operate(self, text, operator, soften=False, c=0):\n",
    "        initial_text = text\n",
    "        initial_wordlen = len(text.split())\n",
    "        text = operator.generate(text, soften=soften)\n",
    "  \n",
    "        if text is None or len(text)<2 or text == initial_text:\n",
    "            text = self.backup.generate(initial_text, soften=soften)\n",
    "            return text\n",
    "\n",
    "        if text and len(text.split()) < initial_wordlen//2 and c < 5:\n",
    "            c += 1\n",
    "            text = self.lose_information_single(initial_text, operator, soften=True, c=c)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def generate(self, text, ops=[\"MostImportantPhraseRemover\", \"DeleteLastSentence\", \"ReplaceNamedEntities\",\n",
    "                                  \"ReplaceNumericalEntities\", \"Pegasus\", \"ReplaceUnits\", \"NegateQuestion\"], \n",
    "                                  **kwargs):\n",
    "        negatives = []\n",
    "        for op in ops:\n",
    "            if op not in self.operations: continue\n",
    "            operator = self.operations[op]\n",
    "            negatives.append(self.operate(text, operator))\n",
    "        return negatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    def __init__(self):\n",
    "        self.positive_generator = PositiveSamples()\n",
    "        self.negative_generator = NegativeSamples()\n",
    "\n",
    "    def generate(self, text, positive_ops=[\"BackTranslate\", \"SameSentence\", \"Num2Words\", \"UnitExpansion\"], \n",
    "                negative_ops=[\"MostImportantPhraseRemover\", \"DeleteLastSentence\", \"ReplaceNamedEntities\",\n",
    "                            \"ReplaceNumericalEntities\", \"Pegasus\", \"ReplaceUnits\", \"NegateQuestion\"]):\n",
    "        positives = self.positive_generator.generate(text, ops=positive_ops)\n",
    "        negatives = self.negative_generator.generate(text, ops=negative_ops)\n",
    "        return positives, negatives"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
